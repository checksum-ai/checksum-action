name: 'PR Review with Progress Tracking'
description: 'Perform PR reviews with Checksum and progress tracking'
inputs:
  track_progress:
    description: 'Enable progress tracking with checkboxes'
    required: false
    default: 'true'
  anthropic_api_key:
    description: 'Anthropic API key for Claude'
    required: true
  app_id:
    description: 'GitHub App ID'
    required: true
  app_private_key:
    description: 'GitHub App private key'
    required: true

runs:
  using: 'composite'
  steps:
    - name: Generate GitHub App token
      id: app-token
      uses: actions/create-github-app-token@v1
      with:
        app-id: ${{ inputs.app_id }}
        private-key: ${{ inputs.app_private_key }}

    - name: PR Review with Progress Tracking
      uses: harsh-sset/checksum-code@main
      with:
        anthropic_api_key: ${{ inputs.anthropic_api_key }}
        track_progress: ${{ inputs.track_progress }}
        github_token: ${{ steps.app-token.outputs.token }}
        prompt: |
          REPO: ${{ github.repository }}
          PR NUMBER: ${{ github.event.pull_request.number }}

          # PR Functional Review - Extremely Comprehensive Functional and Integration Testing

          ## Role and Scope

          - Primary focus: test the code introduced in the PR for functional correctness and realistic integrations.
          - Secondary focus: where the PR clearly touches risky adjacent paths, test those too.
          - Approach: extremely comprehensive - write many compact, focused tests that cover complicated business logic, end-to-end API flows where feasible, and integration points that the PR code orchestrates.

          ## What To Test vs What Not To Test

          - Test: business logic branches, data transformations, cross-function orchestration, retry and backoff logic, pagination, idempotency, concurrency, error handling, auth flows, header filtering, request shaping, response shaping, fixture and test-suite builders, CLI orchestration, build and merge logic for test plans.
          - Do not test: database validations already enforced by SQLAlchemy or the ORM layer, generic framework behavior that is not authored in this repo, low-level library internals.

          ## High-Value Test Heuristics

          - Map PR changes to user-facing workflows and system boundaries; cover start-to-finish paths for both success and failure cases.
          - Validate prerequisites and gating logic (feature flags, verification steps, required settings) and surface missing prerequisites as defects.
          - Stress negative and edge conditions that threaten auth, session, or data integrity flows (e.g., stale tokens, unverified users, malformed payloads).
          - Focus on integration seams the PR wires together; treat third-party internals as black boxes unless the PR overrides them.
          - Avoid restating ORM defaults or framework behavior—only test persistence when the PR customizes it or new logic depends on it.

          ## Coverage Priorities

          - Start with the highest-impact user journeys introduced or modified by the PR; prove the flow end-to-end (initiation → side effects → response) for first-time and returning users.
          - Exercise both success and failure paths for new endpoints, dependencies, and background jobs, including state transitions (e.g., login → token issued → gated resource).
          - Validate integration boundaries the PR wires together: routers calling services, services calling repositories, feature flags toggling behavior.
          - Explicitly test prerequisite or gating flows (e.g., verification steps) so missing prerequisites surface as actionable bugs.
          - Confirm regression coverage for previously broken scenarios before expanding to broader negative cases.

          ## Avoid Low-Signal Tests

          - Do not assert static configuration attributes or constants unless the PR changed them.
          - Skip tests that simply confirm ORM relationship defaults, CRUD generated by third-party libraries, or middleware registration already proven elsewhere.
          - Prefer scenarios that observe behavior (responses, emitted events, state changes) over direct attribute introspection.
          - If a helper exists only to support tests, reconsider the scenario—focus on real system entry points.

          ## Testing Process

          - Use `temp.py` as disposable scratch space for focused, high-signal scenarios.
          - Dedicate most tests to real entry points and business logic (HTTP routes, services, CLI flows) so each scenario observes system behavior, not just data access.
          - Make only trivial changes for testability (e.g., accept dict input instead of a network call). Never change business logic.
          - Use lightweight fakes or in-memory stores to isolate dependencies;
          - Run tests locally with `uv run temp.py`.
          - Note any scenario that cannot be automated and explain the gap in the report.
          - Timebox hands-on execution to roughly 10-20 minutes while staying ruthless about covering high-risk logic.

          ## Number of Tests

          See guidelines to how many tests you should generate. These are just guidelines, feel free to generate many more tests if it's impactful.

          1. A small PR - just a few lines of code changed or a single function is added - generate 5-10 tests
          2. A medium sized PR - Several files changed and a handful of functions where added/changed - generate 10-15 tests
          3. A large PR - significant refactoring or a completly new module of functionality is added - 15-30 tests

          ## Report

          Start with an executive summary - test coverage, main covered funtionality and main bugs detected, and functionality you failed to test

          Continue with the following format per test

          Test Title
          Objective: What business behavior or risk is being validated.
          Functions Exercised: Fully-qualified functions, methods, or endpoints invoked (for example, `src.accounts.services.create_access_token`).
          Inputs & Setup: Concrete parameters, payloads, fixtures, environment variables, and state preparation.
          Steps Executed: Concise ordered steps describing the flow.
          Assertions: Key assertions and why they demonstrate correctness or expose a bug.
          Observed Signals: Relevant outputs, response fields, log excerpts, or database rows supporting the assertions.
          Status: ✅ PASS or ❌ FAIL
          Notes & Follow-ups: Edge cases hit, gaps discovered, remediation ideas.

          ## Examples

          <example id="1" type="agent-har-utilities">
            <code-under-test>
              # agent_like.py - HAR navigation, header filtering, search with pagination
              def list_calls_paginated(ctx, page: int, page_size: int) -> Dict:
                  start = (page - 1) * page_size
                  end = start + page_size
                  total_pages = (len(ctx.entries) + page_size - 1) // page_size

                  calls = []
                  for i, entry in enumerate(ctx.entries[start:end], start=start):
                      url = entry.request_url[:97] + "..." if len(entry.request_url) > 100 else entry.request_url
                      calls.append({"index": i, "method": entry.request_method, "url": url, "status": entry.response_status})

                  return {"calls": calls, "total_pages": total_pages, "current_page": page}

              def get_call_details(ctx, index: int, full_details: bool = False) -> Dict:
                  entry = ctx.entries[index]

                  if full_details:
                      headers = entry.request_headers.copy()
                      if "Cookie" in headers and len(headers["Cookie"]) > 200:
                          headers["Cookie"] = headers["Cookie"][:200] + "...<truncated>"
                      return {"request": {"method": entry.request_method, "headers": headers},
                              "response": {"status": entry.response_status}}
                  else:
                      return {"request": {"headers": list(entry.request_headers.keys())}}

              def search_calls_advanced(ctx, criteria: List[Dict], limit: Optional[int] = None) -> List:
                  matches = []
                  for i, entry in enumerate(ctx.entries):
                      if all(self._matches_criterion(entry, c) for c in criteria):
                          if limit and len(matches) >= limit:
                              matches.append({"index": i, "method": entry.request_method})  # Summary
                          else:
                              matches.append(get_call_details(ctx, i, full_details=True))
                  return matches
            </code-under-test>

            <reasoning-for-test-plan>
              Risk areas:
              1. **Pagination**: Off-by-one errors, total_pages calculation with non-divisible counts
              2. **URL/Cookie Truncation**: Length thresholds (100 for URL, 200 for Cookie)
              3. **Header Filtering**: full_details=True returns values, False returns keys only
              4. **Search Multi-Criterion**: AND logic across criteria with regex/exact matching
              5. **Limit Summarization**: After limit, return summaries not full details

              Test Plan: Pagination boundaries, long URL/cookie truncation, outline vs full mode, multi-criterion search, limit-based summarization
            </reasoning-for-test-plan>

            <testing-code>
              # temp.py - Agent HAR utilities tests
              def test_pagination_and_url_trimming():
                  long_url = "https://api.example.com/" + ("z" * 200)
                  entries = [
                      HarEntry("GET", "https://api.example.com/users", {}, 200),
                      HarEntry("POST", long_url, {}, 201),
                      HarEntry("GET", "https://api.example.com/items", {}, 404),
                  ]
                  ctx = AgentDeps(entries=entries)

                  # Test first page
                  page1 = list_calls_paginated(ctx, page=1, page_size=2)
                  assert page1["total_pages"] == 2 and len(page1["calls"]) == 2

                  # Test URL trimming on page 2
                  page2 = list_calls_paginated(ctx, page=2, page_size=2)
                  assert len(page2["calls"][0]["url"]) <= 100 and page2["calls"][0]["url"].endswith("...")
                  print("✅ Pagination and URL trimming work")

              def test_cookie_truncation_and_header_modes():
                  long_cookie = "session=" + ("x" * 400)
                  entries = [HarEntry("GET", "https://api.example.com", {"Cookie": long_cookie, "Accept": "json"}, 200)]
                  ctx = AgentDeps(entries=entries)

                  # Full details: values with truncation
                  full = get_call_details(ctx, 0, full_details=True)
                  assert len(full["request"]["headers"]["Cookie"]) <= 220
                  assert full["request"]["headers"]["Cookie"].endswith("...<truncated>")

                  # Outline mode: just keys
                  outline = get_call_details(ctx, 0, full_details=False)
                  assert isinstance(outline["request"]["headers"], list)
                  assert "Cookie" in outline["request"]["headers"]
                  print("✅ Cookie truncation and header modes work")

              def test_search_with_limit_summarization():
                  entries = [
                      HarEntry("GET", "https://api.example.com/a", {}, 200),
                      HarEntry("GET", "https://api.example.com/b", {}, 200),
                      HarEntry("POST", "https://api.example.com/c", {}, 201),
                  ]
                  ctx = AgentDeps(entries=entries)

                  # Search GET with limit=1
                  results = search_calls_advanced(ctx, [{"location": "method", "match_type": "exact", "value": "GET"}], limit=1)

                  assert len(results) == 2
                  assert "headers" in results[0]["request"]  # First: full details
                  assert "headers" not in results[1]  # Second: summary only
                  print("✅ Search limit summarization works")

              if __name__ == "__main__":
                  test_pagination_and_url_trimming()
                  test_cookie_truncation_and_header_modes()
                  test_search_with_limit_summarization()
            </testing-code>
          </example>

          <example id="2" type="services-plan-filtering">
            <code-under-test>
              # services_like.py - Test plan filtering, merging, source building
              def filter_plan(new_plan: PlanCode, base_plan: PlanCode, by_name: bool = True) -> PlanCode:
                  if by_name:
                      base_names = {f.name for f in base_plan.fixtures}
                      filtered_fixtures = [f for f in new_plan.fixtures if f.name not in base_names]
                  else:
                      # Filter by content (name + code tuple)
                      base_content = {(f.name, f.code) for f in base_plan.fixtures}
                      filtered_fixtures = [f for f in new_plan.fixtures if (f.name, f.code) not in base_content]

                  return PlanCode(name=new_plan.name, fixtures=filtered_fixtures, tests=filtered_tests)

              def merge_implemented(base: PlanCode, new_cases: List[CaseCode]) -> PlanCode:
                  existing_names = {f.name for f in base.fixtures} | {t.name for t in base.tests}

                  new_fixtures = [c for c in new_cases if c.case_type == "fixture" and c.name not in existing_names]
                  new_tests = [c for c in new_cases if c.case_type == "test" and c.name not in existing_names]

                  return PlanCode(fixtures=base.fixtures + new_fixtures, tests=base.tests + new_tests)

              def build_source(imports: List[str], env_vars: List[EnvVar], cases: List[CaseCode], truncate_envs: bool = False) -> str:
                  lines = imports + [""]

                  if env_vars:
                      lines.append("# Environment setup")
                      for env in env_vars:
                          val = env.var_value[:20] + "...<redacted>..." if truncate_envs and len(env.var_value) > 20 else env.var_value
                          lines.append(f'os.environ["{env.var_name}"] = "{val}"')
                      lines.append("")

                  lines.extend([case.code for case in cases])
                  return "\n".join(lines)
            </code-under-test>

            <reasoning-for-test-plan>
              Risk areas:
              1. **Filter Modes**: by_name=True filters on name only; by_name=False on (name, code) tuple
              2. **Deduplication**: merge_implemented must prevent duplicate fixture/test names
              3. **Case Type Separation**: Fixtures vs tests separated by case_type attribute
              4. **Env Truncation**: build_source truncates env values >20 chars when truncate_envs=True
              5. **Edge Cases**: Empty base plan, all duplicates, content vs name mismatch

              Test Plan: Filter by name vs content, merge deduplication, case type separation, env truncation, empty base edge case
            </reasoning-for-test-plan>

            <testing-code>
              # temp.py - Services plan filtering tests
              def test_filter_by_name_vs_content():
                  base = PlanCode(fixtures=[CaseCode("fx_db", "fixture", "def fx_db(): pass")])
                  new = PlanCode(fixtures=[
                      CaseCode("fx_db", "fixture", "def fx_db(): return 'new'"),  # Same name, diff code
                      CaseCode("fx_cache", "fixture", "def fx_cache(): pass")
                  ])

                  # by_name=True: filters fx_db by name even though code differs
                  filtered_name = filter_plan(new, base, by_name=True)
                  assert len(filtered_name.fixtures) == 1 and filtered_name.fixtures[0].name == "fx_cache"

                  # by_name=False: keeps fx_db because code differs
                  filtered_content = filter_plan(new, base, by_name=False)
                  assert len(filtered_content.fixtures) == 2
                  print("✅ Filter by name vs content works")

              def test_merge_dedup_and_type_separation():
                  base = PlanCode(fixtures=[CaseCode("fx_db", "fixture", "code")], tests=[])
                  new_cases = [
                      CaseCode("fx_db", "fixture", "new_code"),  # Duplicate name
                      CaseCode("fx_cache", "fixture", "code"),
                      CaseCode("test_one", "test", "code"),
                  ]

                  merged = merge_implemented(base, new_cases)

                  # Should dedupe fx_db and separate by type
                  assert len(merged.fixtures) == 2 and {f.name for f in merged.fixtures} == {"fx_db", "fx_cache"}
                  assert len(merged.tests) == 1 and merged.tests[0].name == "test_one"
                  print("✅ Merge deduplication and type separation work")

              def test_build_source_with_env_truncation():
                  long_secret = "a" * 100
                  source = build_source(
                      imports=["import pytest"],
                      env_vars=[EnvVar("SECRET", long_secret), EnvVar("SHORT", "ok")],
                      cases=[CaseCode("test_x", "test", "def test_x(): pass")],
                      truncate_envs=True
                  )

                  assert "...<redacted>..." in source
                  assert long_secret not in source
                  assert 'os.environ["SHORT"] = "ok"' in source
                  print("✅ Env truncation works")

              if __name__ == "__main__":
                  test_filter_by_name_vs_content()
                  test_merge_dedup_and_type_separation()
                  test_build_source_with_env_truncation()
            </testing-code>
          </example>

          <example id="3" type="cli-orchestration">
            <code-under-test>
              # cli_like.py - Async orchestration with subprocess pytest execution
              async def orchestrate_generation(fetch_context: Callable, build_source: Callable, names: Optional[List[str]] = None, run_tests: bool = True) -> str:
                  ctx = await fetch_context()

                  # Filter tests by name if specified
                  tests = [t for t in ctx.plan_code.tests if t.name in names] if names else ctx.plan_code.tests
                  source = build_source(ctx.plan_code.imports, ctx.plan_code.env_vars, ctx.plan_code.fixtures + tests, truncate_envs=False)

                  if run_tests:
                      with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                          f.write(source)
                          temp_path = f.name

                      try:
                          result = subprocess.run(["pytest", temp_path, "-v", "--json-report", "--json-report-file=/tmp/report.json"], capture_output=True, timeout=30)
                          report_path = Path("/tmp/report.json")

                          if report_path.exists():
                              with open(report_path) as rf:
                                  pytest_report = json.load(rf)
                              summary = {"status": "completed", "exit_code": result.returncode, "passed": pytest_report.get("summary", {}).get("passed", 0)}
                          else:
                              summary = {"status": "completed", "exit_code": result.returncode, "stdout": result.stdout[:500]}
                      finally:
                          Path(temp_path).unlink(missing_ok=True)
                  else:
                      summary = {"status": "generated", "tests_count": len(tests), "source_lines": len(source.split('\n'))}

                  return json.dumps(summary, indent=2)

              def run_pytest_isolated(test_file: str, markers: Optional[List[str]] = None) -> dict:
                  cmd = ["pytest", test_file, "-v"]
                  if markers:
                      for m in markers:
                          cmd.extend(["-m", m])

                  result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)

                  # Parse pass/fail counts from stdout
                  passed = failed = 0
                  for line in result.stdout.split('\n'):
                      if ' passed' in line:
                          parts = line.split()
                          passed = int(parts[parts.index('passed') - 1]) if 'passed' in parts else 0
                      if ' failed' in line:
                          parts = line.split()
                          failed = int(parts[parts.index('failed') - 1]) if 'failed' in parts else 0

                  return {"exit_code": result.returncode, "passed": passed, "failed": failed, "stdout": result.stdout}
            </code-under-test>

            <reasoning-for-test-plan>
              Risk areas:
              1. **Async Context Fetching**: Must await fetch_context() correctly
              2. **Name Filtering**: When names provided, filter tests to only those in list
              3. **Subprocess Execution**: pytest subprocess with timeout, JSON report parsing
              4. **Temp File Cleanup**: Must clean up temp files in finally block
              5. **Output Parsing**: Extract pass/fail counts from pytest stdout
              6. **Marker Filtering**: pytest -m flag must be passed correctly

              Test Plan: Async orchestration, name filtering, pytest subprocess with JSON report, temp file cleanup, marker filtering, output parsing
            </reasoning-for-test-plan>

            <testing-code>
              # temp.py - CLI orchestration tests
              import asyncio, json, tempfile
              from pathlib import Path
              from unittest.mock import patch, MagicMock

              async def fake_fetch_context():
                  return type("Ctx", (), {"plan_code": type("Plan", (), {"tests": [{"name": "test_a"}, {"name": "test_b"}], "fixtures": [], "imports": [], "env_vars": []})()})()

              def mock_build_source(imports, env_vars, cases, truncate_envs=False):
                  return "import pytest\n\ndef test_example(): assert True"

              def test_orchestrate_with_name_filtering():
                  async def run():
                      result_json = await orchestrate_generation(fake_fetch_context, mock_build_source, names=["test_a"], run_tests=False)
                      result = json.loads(result_json)
                      assert result["status"] == "generated" and result["tests_count"] == 1
                      print("✅ Name filtering works")

                  asyncio.run(run())

              @patch('subprocess.run')
              def test_orchestrate_with_pytest_mock(mock_subprocess):
                  mock_subprocess.return_value = MagicMock(returncode=0, stdout="2 passed")

                  async def run():
                      with patch('pathlib.Path.exists', return_value=True), patch('builtins.open', create=True) as mock_open:
                          mock_open.return_value.__enter__.return_value.read.return_value = json.dumps({"summary": {"passed": 2, "failed": 0}})
                          result_json = await orchestrate_generation(fake_fetch_context, mock_build_source, run_tests=True)

                      result = json.loads(result_json)
                      assert result["status"] == "completed" and result["passed"] == 2
                      print("✅ Pytest execution with JSON report works")

                  asyncio.run(run())

              def test_run_pytest_with_markers():
                  test_code = 'import pytest\n\n@pytest.mark.fast\ndef test_x(): assert True\n\n@pytest.mark.slow\ndef test_y(): assert True'

                  with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                      f.write(test_code)
                      temp_path = f.name

                  try:
                      result = run_pytest_isolated(temp_path, markers=["fast"])
                      assert result["passed"] == 1 and "test_x" in result["stdout"]
                      print("✅ Marker filtering works")
                  finally:
                      Path(temp_path).unlink(missing_ok=True)

              if __name__ == "__main__":
                  test_orchestrate_with_name_filtering()
                  test_orchestrate_with_pytest_mock()
                  test_run_pytest_with_markers()
            </testing-code>
          </example>

          ## Workflow

              1.	Review the Pull Request changes.
              2.	Identify the most complex and risky units that the PR touches.
              3.	Enumerate at high-signal tests covering success paths, failure paths, and prerequisite gaps; track them as a todo list.
              4.	Run the tests locally with `uv run temp.py`.
              5.	Capture the structured report: summary first, then strategy, environment, bug list, and per-test sections.
              6.	Clean up

          Minimum bar: produce at least 20 focused tests; expand coverage further when risk warrants it.

          Be comprehensive and diligent. ULTRATHINK, THINK HARD AND MAKE SURE YOU FULLY TEST THE MOST IMPORTANT WORKFLOWS OF THE CODE THE USER WROTE.

        claude_args: |
          --allowedTools "mcp__github_inline_comment__create_inline_comment,Bash(gh pr comment:*),Bash(gh pr diff:*),Bash(gh pr view:*)"