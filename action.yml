name: 'PR Review with Progress Tracking'
description: 'Perform PR reviews with Checksum and progress tracking'
inputs:
  track_progress:
    description: 'Enable progress tracking with checkboxes'
    required: false
    default: 'true'
  anthropic_api_key:
    description: 'Anthropic API key for Claude'
    required: true
  app_id:
    description: 'Client ID'
    required: true
  app_private_key:
    description: 'Client secret'
    required: true

runs:
  using: 'composite'
  steps:
    - name: Fetch authentication credentials
      id: auth-credentials
      shell: bash
      run: |
        response=$(curl -sS -X POST https://auth-mw-770070026559.us-central1.run.app \
          -H "Content-Type: application/json" \
          -d '{"app_id": "${{ inputs.app_id }}", "app_key": "${{ inputs.app_private_key }}"}')
        
        # Extract values from response
        gh_id=$(echo "$response" | jq -r '.gh_id')
        gh_key_raw=$(echo "$response" | jq -r '.gh_key')
        
        # Convert escaped newlines to actual newlines in the private key
        gh_key=$(echo "$gh_key_raw" | sed 's/\\n/\n/g')
        
        # Mask sensitive values from logs
        echo "::add-mask::$gh_id"
        # Mask each line of the private key separately
        while IFS= read -r line; do
          [[ -n "$line" ]] && echo "::add-mask::$line"
        done <<< "$gh_key"
        # Also mask common private key patterns to catch any missed lines
        echo "::add-mask::BEGIN"
        echo "::add-mask::END"
        echo "::add-mask::PRIVATE KEY"
        
        # Set outputs for use in subsequent steps
        echo "gh_id=$gh_id" >> $GITHUB_OUTPUT
        echo "gh_key<<EOF" >> $GITHUB_OUTPUT
        echo "$gh_key" >> $GITHUB_OUTPUT
        echo "EOF" >> $GITHUB_OUTPUT

    - name: Generate GitHub App token
      id: app-token
      uses: actions/create-github-app-token@v1
      with:
        app-id: ${{ steps.auth-credentials.outputs.gh_id }}
        private-key: ${{ steps.auth-credentials.outputs.gh_key }}

    - name: Generate GitHub Clone token
      id: clone-token
      uses: actions/create-github-app-token@v1
      with:
        app-id: ${{ steps.auth-credentials.outputs.gh_id }}
        private-key: ${{ steps.auth-credentials.outputs.gh_key }}
        owner: harsh-sset
        repository: checksum-code-priv
    
    - name: Check out private action repo
      uses: actions/checkout@v4
      with:
        repository: harsh-sset/checksum-code-priv
        token: ${{ steps.clone-token.outputs.token }} # Use the secret you created
        path: ./.github/private-actions/checksum-code # Check it out to a specific path
    - name: PR Review with Progress Tracking
      uses: ./.github/private-actions/checksum-code
      with:
        app_id: ${{ inputs.app_id }}
        app_private_key: ${{ inputs.app_private_key }}
        track_progress: ${{ inputs.track_progress }}
        prompt: |
          REPO: ${{ github.repository }}
          PR NUMBER: ${{ github.event.pull_request.number }}

          # PR Functional Review - Extremely Comprehensive Functional and Integration Testing

          ## Role and Scope

          - Primary focus: test the code introduced in the PR for functional correctness and realistic integrations.
          - Secondary focus: where the PR clearly touches risky adjacent paths, test those too.
          - Approach: extremely comprehensive - write many compact, focused tests that cover complicated business logic, end-to-end API flows where feasible, and integration points that the PR code orchestrates.

          ## What To Test vs What Not To Test

          - Test: business logic branches, data transformations, cross-function orchestration, retry and backoff logic, pagination, idempotency, concurrency, error handling, auth flows, header filtering, request shaping, response shaping, fixture and test-suite builders, CLI orchestration, build and merge logic for test plans.
          - Do not test: database validations already enforced by SQLAlchemy or the ORM layer, generic framework behavior that is not authored in this repo, low-level library internals.

          ## High-Value Test Heuristics

          - Map PR changes to user-facing workflows and system boundaries; cover start-to-finish paths for both success and failure cases.
          - Validate prerequisites and gating logic (feature flags, verification steps, required settings) and surface missing prerequisites as defects.
          - Stress negative and edge conditions that threaten auth, session, or data integrity flows (e.g., stale tokens, unverified users, malformed payloads).
          - Focus on integration seams the PR wires together; treat third-party internals as black boxes unless the PR overrides them.
          - Avoid restating ORM defaults or framework behavior—only test persistence when the PR customizes it or new logic depends on it.

          ## Coverage Priorities

          - Start with the highest-impact user journeys introduced or modified by the PR; prove the flow end-to-end (initiation → side effects → response) for first-time and returning users.
          - Exercise both success and failure paths for new endpoints, dependencies, and background jobs, including state transitions (e.g., login → token issued → gated resource).
          - Validate integration boundaries the PR wires together: routers calling services, services calling repositories, feature flags toggling behavior.
          - Explicitly test prerequisite or gating flows (e.g., verification steps) so missing prerequisites surface as actionable bugs.
          - Confirm regression coverage for previously broken scenarios before expanding to broader negative cases.

          ## Avoid Low-Signal Tests

          - Do not assert static configuration attributes or constants unless the PR changed them.
          - Skip tests that simply confirm ORM relationship defaults, CRUD generated by third-party libraries, or middleware registration already proven elsewhere.
          - Prefer scenarios that observe behavior (responses, emitted events, state changes) over direct attribute introspection.
          - If a helper exists only to support tests, reconsider the scenario—focus on real system entry points.

          ## Testing Process

          - Use `temp.py` as disposable scratch space for focused, high-signal scenarios.
          - Dedicate most tests to real entry points and business logic (HTTP routes, services, CLI flows) so each scenario observes system behavior, not just data access.
          - Make only trivial changes for testability (e.g., accept dict input instead of a network call). Never change business logic.
          - Use lightweight fakes or in-memory stores to isolate dependencies;
          - Run tests locally with `uv run temp.py`.
          - Note any scenario that cannot be automated and explain the gap in the report.
          - Timebox hands-on execution to roughly 10-20 minutes while staying ruthless about covering high-risk logic.

          ## Number of Tests

          See guidelines to how many tests you should generate. These are just guidelines, feel free to generate many more tests if it's impactful.

          1. A small PR - just a few lines of code changed or a single function is added - generate 5-10 tests
          2. A medium sized PR - Several files changed and a handful of functions where added/changed - generate 10-15 tests
          3. A large PR - significant refactoring or a completly new module of functionality is added - 15-30 tests

          ## Report

          Start with an executive summary - test coverage, main covered funtionality and main bugs detected, and functionality you failed to test

          Continue with the following format per test

          Test Title
          Objective: What business behavior or risk is being validated.
          Functions Exercised: Fully-qualified functions, methods, or endpoints invoked (for example, `src.accounts.services.create_access_token`).
          Inputs & Setup: Concrete parameters, payloads, fixtures, environment variables, and state preparation.
          Steps Executed: Concise ordered steps describing the flow.
          Assertions: Key assertions and why they demonstrate correctness or expose a bug.
          Observed Signals: Relevant outputs, response fields, log excerpts, or database rows supporting the assertions.
          Status: ✅ PASS or ❌ FAIL
          Notes & Follow-ups: Edge cases hit, gaps discovered, remediation ideas.

          ## Workflow

              1.	Review the Pull Request changes.
              2.	Identify the most complex and risky units that the PR touches.
              3.	Enumerate at high-signal tests covering success paths, failure paths, and prerequisite gaps; track them as a todo list.
              4.	Run the tests locally with `uv run temp.py`.
              5.	Capture the structured report: summary first, then strategy, environment, bug list, and per-test sections.
              6.	Clean up

          Minimum bar: produce at least 20 focused tests; expand coverage further when risk warrants it.

          When the tests are generated execute them locally with code execution tool and bash tools to verify they are working as expected.

          Be comprehensive and diligent. ULTRATHINK, THINK HARD AND MAKE SURE YOU FULLY TEST THE MOST IMPORTANT WORKFLOWS OF THE CODE THE USER WROTE.

        claude_args: |
          --allowedTools "mcp__github_inline_comment__create_inline_comment,Bash(gh pr comment:*),Bash(gh pr diff:*),Bash(gh pr view:*),code_execution_20250825,Bash(*),Bash"
